{"ast":null,"code":"'use strict';\n\nvar TransformStream = require('stream').Transform,\n    DevNullStream = require('./dev_null_stream'),\n    inherits = require('util').inherits,\n    Tokenizer = require('../tokenizer'),\n    LocationInfoTokenizerMixin = require('../extensions/location_info/tokenizer_mixin'),\n    ParserFeedbackSimulator = require('./parser_feedback_simulator'),\n    mergeOptions = require('../utils/merge_options');\n\nvar DEFAULT_OPTIONS = {\n  locationInfo: false\n};\n\nvar SAXParser = module.exports = function (options) {\n  TransformStream.call(this);\n  this.options = mergeOptions(DEFAULT_OPTIONS, options);\n  this.tokenizer = new Tokenizer(options);\n  if (this.options.locationInfo) new LocationInfoTokenizerMixin(this.tokenizer);\n  this.parserFeedbackSimulator = new ParserFeedbackSimulator(this.tokenizer);\n  this.pendingText = null;\n  this.currentTokenLocation = void 0;\n  this.lastChunkWritten = false;\n  this.stopped = false; // NOTE: always pipe stream to the /dev/null stream to avoid\n  // `highWaterMark` hit even if we don't have consumers.\n  // (see: https://github.com/inikulin/parse5/issues/97#issuecomment-171940774)\n\n  this.pipe(new DevNullStream());\n};\n\ninherits(SAXParser, TransformStream); //TransformStream implementation\n\nSAXParser.prototype._transform = function (chunk, encoding, callback) {\n  if (!this.stopped) {\n    this.tokenizer.write(chunk.toString('utf8'), this.lastChunkWritten);\n\n    this._runParsingLoop();\n  }\n\n  this.push(chunk);\n  callback();\n};\n\nSAXParser.prototype._flush = function (callback) {\n  callback();\n};\n\nSAXParser.prototype.end = function (chunk, encoding, callback) {\n  this.lastChunkWritten = true;\n  TransformStream.prototype.end.call(this, chunk, encoding, callback);\n};\n\nSAXParser.prototype.stop = function () {\n  this.stopped = true;\n}; //Internals\n\n\nSAXParser.prototype._runParsingLoop = function () {\n  do {\n    var token = this.parserFeedbackSimulator.getNextToken();\n    if (token.type === Tokenizer.HIBERNATION_TOKEN) break;\n\n    if (token.type === Tokenizer.CHARACTER_TOKEN || token.type === Tokenizer.WHITESPACE_CHARACTER_TOKEN || token.type === Tokenizer.NULL_CHARACTER_TOKEN) {\n      if (this.options.locationInfo) {\n        if (this.pendingText === null) this.currentTokenLocation = token.location;else this.currentTokenLocation.endOffset = token.location.endOffset;\n      }\n\n      this.pendingText = (this.pendingText || '') + token.chars;\n    } else {\n      this._emitPendingText();\n\n      this._handleToken(token);\n    }\n  } while (!this.stopped && token.type !== Tokenizer.EOF_TOKEN);\n};\n\nSAXParser.prototype._handleToken = function (token) {\n  if (this.options.locationInfo) this.currentTokenLocation = token.location;\n  if (token.type === Tokenizer.START_TAG_TOKEN) this.emit('startTag', token.tagName, token.attrs, token.selfClosing, this.currentTokenLocation);else if (token.type === Tokenizer.END_TAG_TOKEN) this.emit('endTag', token.tagName, this.currentTokenLocation);else if (token.type === Tokenizer.COMMENT_TOKEN) this.emit('comment', token.data, this.currentTokenLocation);else if (token.type === Tokenizer.DOCTYPE_TOKEN) this.emit('doctype', token.name, token.publicId, token.systemId, this.currentTokenLocation);\n};\n\nSAXParser.prototype._emitPendingText = function () {\n  if (this.pendingText !== null) {\n    this.emit('text', this.pendingText, this.currentTokenLocation);\n    this.pendingText = null;\n  }\n};","map":{"version":3,"sources":["/Users/lambda_school_loaner_244/Desktop/LAMBDA/projects/projectsCollection/node_modules/parse5/lib/sax/index.js"],"names":["TransformStream","require","Transform","DevNullStream","inherits","Tokenizer","LocationInfoTokenizerMixin","ParserFeedbackSimulator","mergeOptions","DEFAULT_OPTIONS","locationInfo","SAXParser","module","exports","options","call","tokenizer","parserFeedbackSimulator","pendingText","currentTokenLocation","lastChunkWritten","stopped","pipe","prototype","_transform","chunk","encoding","callback","write","toString","_runParsingLoop","push","_flush","end","stop","token","getNextToken","type","HIBERNATION_TOKEN","CHARACTER_TOKEN","WHITESPACE_CHARACTER_TOKEN","NULL_CHARACTER_TOKEN","location","endOffset","chars","_emitPendingText","_handleToken","EOF_TOKEN","START_TAG_TOKEN","emit","tagName","attrs","selfClosing","END_TAG_TOKEN","COMMENT_TOKEN","data","DOCTYPE_TOKEN","name","publicId","systemId"],"mappings":"AAAA;;AAEA,IAAIA,eAAe,GAAGC,OAAO,CAAC,QAAD,CAAP,CAAkBC,SAAxC;AAAA,IACIC,aAAa,GAAGF,OAAO,CAAC,mBAAD,CAD3B;AAAA,IAEIG,QAAQ,GAAGH,OAAO,CAAC,MAAD,CAAP,CAAgBG,QAF/B;AAAA,IAGIC,SAAS,GAAGJ,OAAO,CAAC,cAAD,CAHvB;AAAA,IAIIK,0BAA0B,GAAGL,OAAO,CAAC,6CAAD,CAJxC;AAAA,IAKIM,uBAAuB,GAAGN,OAAO,CAAC,6BAAD,CALrC;AAAA,IAMIO,YAAY,GAAGP,OAAO,CAAC,wBAAD,CAN1B;;AAQA,IAAIQ,eAAe,GAAG;AAClBC,EAAAA,YAAY,EAAE;AADI,CAAtB;;AAIA,IAAIC,SAAS,GAAGC,MAAM,CAACC,OAAP,GAAiB,UAAUC,OAAV,EAAmB;AAChDd,EAAAA,eAAe,CAACe,IAAhB,CAAqB,IAArB;AAEA,OAAKD,OAAL,GAAeN,YAAY,CAACC,eAAD,EAAkBK,OAAlB,CAA3B;AAEA,OAAKE,SAAL,GAAiB,IAAIX,SAAJ,CAAcS,OAAd,CAAjB;AAEA,MAAI,KAAKA,OAAL,CAAaJ,YAAjB,EACI,IAAIJ,0BAAJ,CAA+B,KAAKU,SAApC;AAEJ,OAAKC,uBAAL,GAA+B,IAAIV,uBAAJ,CAA4B,KAAKS,SAAjC,CAA/B;AAEA,OAAKE,WAAL,GAAmB,IAAnB;AACA,OAAKC,oBAAL,GAA4B,KAAK,CAAjC;AAEA,OAAKC,gBAAL,GAAwB,KAAxB;AACA,OAAKC,OAAL,GAAe,KAAf,CAhBgD,CAkBhD;AACA;AACA;;AACA,OAAKC,IAAL,CAAU,IAAInB,aAAJ,EAAV;AACH,CAtBD;;AAwBAC,QAAQ,CAACO,SAAD,EAAYX,eAAZ,CAAR,C,CAEA;;AACAW,SAAS,CAACY,SAAV,CAAoBC,UAApB,GAAiC,UAAUC,KAAV,EAAiBC,QAAjB,EAA2BC,QAA3B,EAAqC;AAClE,MAAI,CAAC,KAAKN,OAAV,EAAmB;AACf,SAAKL,SAAL,CAAeY,KAAf,CAAqBH,KAAK,CAACI,QAAN,CAAe,MAAf,CAArB,EAA6C,KAAKT,gBAAlD;;AACA,SAAKU,eAAL;AACH;;AAED,OAAKC,IAAL,CAAUN,KAAV;AAEAE,EAAAA,QAAQ;AACX,CATD;;AAWAhB,SAAS,CAACY,SAAV,CAAoBS,MAApB,GAA6B,UAAUL,QAAV,EAAoB;AAC7CA,EAAAA,QAAQ;AACX,CAFD;;AAIAhB,SAAS,CAACY,SAAV,CAAoBU,GAApB,GAA0B,UAAUR,KAAV,EAAiBC,QAAjB,EAA2BC,QAA3B,EAAqC;AAC3D,OAAKP,gBAAL,GAAwB,IAAxB;AACApB,EAAAA,eAAe,CAACuB,SAAhB,CAA0BU,GAA1B,CAA8BlB,IAA9B,CAAmC,IAAnC,EAAyCU,KAAzC,EAAgDC,QAAhD,EAA0DC,QAA1D;AACH,CAHD;;AAKAhB,SAAS,CAACY,SAAV,CAAoBW,IAApB,GAA2B,YAAY;AACnC,OAAKb,OAAL,GAAe,IAAf;AACH,CAFD,C,CAIA;;;AACAV,SAAS,CAACY,SAAV,CAAoBO,eAApB,GAAsC,YAAY;AAC9C,KAAG;AACC,QAAIK,KAAK,GAAG,KAAKlB,uBAAL,CAA6BmB,YAA7B,EAAZ;AAEA,QAAID,KAAK,CAACE,IAAN,KAAehC,SAAS,CAACiC,iBAA7B,EACI;;AAEJ,QAAIH,KAAK,CAACE,IAAN,KAAehC,SAAS,CAACkC,eAAzB,IACAJ,KAAK,CAACE,IAAN,KAAehC,SAAS,CAACmC,0BADzB,IAEAL,KAAK,CAACE,IAAN,KAAehC,SAAS,CAACoC,oBAF7B,EAEmD;AAE/C,UAAI,KAAK3B,OAAL,CAAaJ,YAAjB,EAA+B;AAC3B,YAAI,KAAKQ,WAAL,KAAqB,IAAzB,EACI,KAAKC,oBAAL,GAA4BgB,KAAK,CAACO,QAAlC,CADJ,KAII,KAAKvB,oBAAL,CAA0BwB,SAA1B,GAAsCR,KAAK,CAACO,QAAN,CAAeC,SAArD;AACP;;AAED,WAAKzB,WAAL,GAAmB,CAAC,KAAKA,WAAL,IAAoB,EAArB,IAA2BiB,KAAK,CAACS,KAApD;AACH,KAbD,MAeK;AACD,WAAKC,gBAAL;;AACA,WAAKC,YAAL,CAAkBX,KAAlB;AACH;AACJ,GAzBD,QAyBS,CAAC,KAAKd,OAAN,IAAiBc,KAAK,CAACE,IAAN,KAAehC,SAAS,CAAC0C,SAzBnD;AA0BH,CA3BD;;AA6BApC,SAAS,CAACY,SAAV,CAAoBuB,YAApB,GAAmC,UAAUX,KAAV,EAAiB;AAChD,MAAI,KAAKrB,OAAL,CAAaJ,YAAjB,EACI,KAAKS,oBAAL,GAA4BgB,KAAK,CAACO,QAAlC;AAEJ,MAAIP,KAAK,CAACE,IAAN,KAAehC,SAAS,CAAC2C,eAA7B,EACI,KAAKC,IAAL,CAAU,UAAV,EAAsBd,KAAK,CAACe,OAA5B,EAAqCf,KAAK,CAACgB,KAA3C,EAAkDhB,KAAK,CAACiB,WAAxD,EAAqE,KAAKjC,oBAA1E,EADJ,KAGK,IAAIgB,KAAK,CAACE,IAAN,KAAehC,SAAS,CAACgD,aAA7B,EACD,KAAKJ,IAAL,CAAU,QAAV,EAAoBd,KAAK,CAACe,OAA1B,EAAmC,KAAK/B,oBAAxC,EADC,KAGA,IAAIgB,KAAK,CAACE,IAAN,KAAehC,SAAS,CAACiD,aAA7B,EACD,KAAKL,IAAL,CAAU,SAAV,EAAqBd,KAAK,CAACoB,IAA3B,EAAiC,KAAKpC,oBAAtC,EADC,KAGA,IAAIgB,KAAK,CAACE,IAAN,KAAehC,SAAS,CAACmD,aAA7B,EACD,KAAKP,IAAL,CAAU,SAAV,EAAqBd,KAAK,CAACsB,IAA3B,EAAiCtB,KAAK,CAACuB,QAAvC,EAAiDvB,KAAK,CAACwB,QAAvD,EAAiE,KAAKxC,oBAAtE;AACP,CAfD;;AAiBAR,SAAS,CAACY,SAAV,CAAoBsB,gBAApB,GAAuC,YAAY;AAC/C,MAAI,KAAK3B,WAAL,KAAqB,IAAzB,EAA+B;AAC3B,SAAK+B,IAAL,CAAU,MAAV,EAAkB,KAAK/B,WAAvB,EAAoC,KAAKC,oBAAzC;AACA,SAAKD,WAAL,GAAmB,IAAnB;AACH;AACJ,CALD","sourcesContent":["'use strict';\n\nvar TransformStream = require('stream').Transform,\n    DevNullStream = require('./dev_null_stream'),\n    inherits = require('util').inherits,\n    Tokenizer = require('../tokenizer'),\n    LocationInfoTokenizerMixin = require('../extensions/location_info/tokenizer_mixin'),\n    ParserFeedbackSimulator = require('./parser_feedback_simulator'),\n    mergeOptions = require('../utils/merge_options');\n\nvar DEFAULT_OPTIONS = {\n    locationInfo: false\n};\n\nvar SAXParser = module.exports = function (options) {\n    TransformStream.call(this);\n\n    this.options = mergeOptions(DEFAULT_OPTIONS, options);\n\n    this.tokenizer = new Tokenizer(options);\n\n    if (this.options.locationInfo)\n        new LocationInfoTokenizerMixin(this.tokenizer);\n\n    this.parserFeedbackSimulator = new ParserFeedbackSimulator(this.tokenizer);\n\n    this.pendingText = null;\n    this.currentTokenLocation = void 0;\n\n    this.lastChunkWritten = false;\n    this.stopped = false;\n\n    // NOTE: always pipe stream to the /dev/null stream to avoid\n    // `highWaterMark` hit even if we don't have consumers.\n    // (see: https://github.com/inikulin/parse5/issues/97#issuecomment-171940774)\n    this.pipe(new DevNullStream());\n};\n\ninherits(SAXParser, TransformStream);\n\n//TransformStream implementation\nSAXParser.prototype._transform = function (chunk, encoding, callback) {\n    if (!this.stopped) {\n        this.tokenizer.write(chunk.toString('utf8'), this.lastChunkWritten);\n        this._runParsingLoop();\n    }\n\n    this.push(chunk);\n\n    callback();\n};\n\nSAXParser.prototype._flush = function (callback) {\n    callback();\n};\n\nSAXParser.prototype.end = function (chunk, encoding, callback) {\n    this.lastChunkWritten = true;\n    TransformStream.prototype.end.call(this, chunk, encoding, callback);\n};\n\nSAXParser.prototype.stop = function () {\n    this.stopped = true;\n};\n\n//Internals\nSAXParser.prototype._runParsingLoop = function () {\n    do {\n        var token = this.parserFeedbackSimulator.getNextToken();\n\n        if (token.type === Tokenizer.HIBERNATION_TOKEN)\n            break;\n\n        if (token.type === Tokenizer.CHARACTER_TOKEN ||\n            token.type === Tokenizer.WHITESPACE_CHARACTER_TOKEN ||\n            token.type === Tokenizer.NULL_CHARACTER_TOKEN) {\n\n            if (this.options.locationInfo) {\n                if (this.pendingText === null)\n                    this.currentTokenLocation = token.location;\n\n                else\n                    this.currentTokenLocation.endOffset = token.location.endOffset;\n            }\n\n            this.pendingText = (this.pendingText || '') + token.chars;\n        }\n\n        else {\n            this._emitPendingText();\n            this._handleToken(token);\n        }\n    } while (!this.stopped && token.type !== Tokenizer.EOF_TOKEN);\n};\n\nSAXParser.prototype._handleToken = function (token) {\n    if (this.options.locationInfo)\n        this.currentTokenLocation = token.location;\n\n    if (token.type === Tokenizer.START_TAG_TOKEN)\n        this.emit('startTag', token.tagName, token.attrs, token.selfClosing, this.currentTokenLocation);\n\n    else if (token.type === Tokenizer.END_TAG_TOKEN)\n        this.emit('endTag', token.tagName, this.currentTokenLocation);\n\n    else if (token.type === Tokenizer.COMMENT_TOKEN)\n        this.emit('comment', token.data, this.currentTokenLocation);\n\n    else if (token.type === Tokenizer.DOCTYPE_TOKEN)\n        this.emit('doctype', token.name, token.publicId, token.systemId, this.currentTokenLocation);\n};\n\nSAXParser.prototype._emitPendingText = function () {\n    if (this.pendingText !== null) {\n        this.emit('text', this.pendingText, this.currentTokenLocation);\n        this.pendingText = null;\n    }\n};\n"]},"metadata":{},"sourceType":"script"}